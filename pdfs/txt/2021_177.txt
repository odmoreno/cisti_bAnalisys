Personalizing the explanation extraction in Intelligent Decision Support Systems
ABSTRACT
The use of black-box models compromises the adoption of Decision Support Systems because they tipically do not allow the decision maker to understand how decisions are issued by the system. Even with the growth of the Explanable Artificial Intelligence area, little has been done to personalize the explanations generated in the context of these systems. This article presents an approach to customize explanations of decisions using a graph to mediate inferences. Proofs of concept and analysis of simulated decisions are presented. The results suggest that the approach provided the systems with the ability to issue satisfactory explanations, customizing them for simulated user profiles as long as reasonable levels of accuracy were maintained.
